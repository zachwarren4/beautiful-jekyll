---
layout: post
title: "Recreating the results of the Galaxy Zoo compeition"
date: 2017-07-18
---

At the beginning of the summer, I set out with a somewhat fuzzy goal of 'creating a galaxy classifier with a neural net.' Since then, I've learned a lot, though I haven't made a post in a awhile, I've been working hard on replicating the results of the top participant in the competition, Sander Dieleman. His [solution](https://github.com/benanne/kaggle-galaxies) was written over three years ago and much has changed since. Instead of using Theano and pylearn2, as he did, I used Keras, a much simpler front end for writing NNs. 

## The challenge
The original source of the challenge is [here](https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge). The goal was to classify galaxies based on the Galaxy Zoo decision tree. Rather than just a simple binary/categorical classifier, the model is supposed to predict the percentage of 'yes' answers to the following questions from galaxies previously classified by humans on Galaxy Zoo. 

* Q1. Is the object a smooth galaxy, a galaxy with features/disk or a star?
* Q2. Is it edge-on?
* Q3. Is there a bar? 
* Q4. Is there a spiral pattern?
* Q5. How prominent is the central bulge?
* Q6. Is there anything "odd" about the galaxy?
* Q7. How round is the smooth galaxy? 
* Q8. What is the odd feature? 
* Q9. What shape is the bulge in the edge-on galaxy?
* Q10. How tightly wound are the spiral arms?
* Q11. How many spiral arms are there?

This made designing the model a little bit more difficult than just returning a normal one-hot vector like a normal image classifier.

## The Process

After the last post of dissecting the ConvNet, I actually didn't have too difficult a time deciding on a model for my classifier. It took a bit of fiddling but I ended up with a model similar to Dieleman's though, since I have a bit more computing power, I made my convolutional layers and dense layers a little bigger.

Dieleman's solution also included quite a bit of image manipulation. He rotated, shifted, downsampled, scaled, etc in order to make his dataset as complete as possible. It took him almost 1000 lines of code to set up a system to do this in a way that actively augmented the data in realtime. With Keras, it took 11. Keras provides the ```ImageDataGenerator()``` class which takes parameters allowing for rotation, shift, zoom, etc as found [here](https://keras.io/preprocessing/image/). Here is the generator I used:

```python
datagen = ImageDataGenerator(
    rescale = 1./255,
    rotation_range=360,
    width_shift_range=0.05,
    height_shift_range=0.05,
    shear_range=0.05,
    zoom_range=0.1,
    horizontal_flip=False,
    fill_mode='nearest')
```

I rescaled to values between 0-1 that the NN could properly interpret, allowed for any amount of rotation, allowed for 5% shift in any direction as most galaxies are the centerpiece of their photograph. I also allowed for a small amount of shear which is, simply put, streching in some direction and 10% zoom, because some galaxies of similar type would be different sizes in pixels depending on the image. A small amount of zoom corrects for this. ```fill_mode``` just decides how to fill the new pixels generated by shifting or zooming an image in order to keep it the same shape. 

After creating the generator, it took two lines of code to create the generators for each dataset, the training and validation sets. 

```python
train_gen = datagen.flow(train_x, train_y, batch_size=32)
val_gen = datagen.flow(val_x, val_y, batch_size=16)
```

## The model

I'll have the code available on github after I clean it up so instead of pasting a code block I'll just describe the model.

I had four convolutional layers with pooling and two dense layers, each with dropout. The convolutional layers had 64, 128, 256, and 512 filters respectively. Each filter performs some operation on the image like sharpening, blurring, edge detection. I chose the standard 3x3 for the kernel size of the layers, which essentially limits the operations the convolution layer can perform, I plan on writing another blog post specifically about that. 

The dense layers each had 4096 neurons with a dropout rate of 50% in order to limit overfitting. I didn't choose 50% for any particular reason other than it seemed to work pretty well. I didn't see any noticable effects of overfitting until ~epoch 80 of the 90 I trained for. 

I compiled the model with mean squared error loss and an RMS propagation optimizer since Kaggle scoring was based on RMS error. 

## The results

After 90 epochs of training which took around an hour and a half, my results weren't as good as Dieleman's, which I expected since he used quite a bit of model averaging. I did end up with an RMSE loss of .0124, which would have scored .124 on Kaggle's leaderboards, putting me in the top half. I accidentally cleared my python kernel before saving the graph of the training, whoops. I did save the model and I am going to try and recreate the results in the next few days, just need to set aside a few hours for my computer to become a space heater. 

## Some interesting things

Something I thought would be interesting but turned out to not be at all was trying to find out what an 'average' galaxy looked like based on the dataset. *Drum roll please*:

![avg_galaxy](/img/average_galaxy.jpg)

So mostly just ridiculous colors. After flipping through enough of the images, this isn't particularly surprising, there is a lot of color in each image that isn't part of the galaxy itself and all the galaxies have different sizes/rotations. Worth a shot anyway.

I also had an accuracy result of only 70%, which at first seemed very poor. After looking around a little, I realized that this score was somewhat meaningless. I wasn't classifying galaxies in a way that could lead to a 'perfect' result. A slightly wrong answer rippled into something that could be only '70%' correct but actually just had only slight inaccuracies at the third decimal place. 

Overall, this project was very interesting, and I plan on writing another post with some more visualization after I clean up my results and reweight them for scoring on Kaggle's system. 
